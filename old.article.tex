%\documentclass[a4paper,10pt]{article}
\documentclass[journal]{IEEEtran}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}

\usepackage{cite} %for citations

 %this is for math typing (eg: cases)
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{epsfig} %for figures

%\usepackage[caption=false]{caption}%for captions
\usepackage[caption=false,font=footnotesize]{subfig} %for subfigures

%opening
\title{ SLC SAR speckle filtering using Homoskedastic effects of Logarithmic Transfomation }

%\author{Thanh-Hai Le, Ian McLoughlin}
\author{Thanh-Hai~Le,
        Ian~McLoughlin,~\IEEEmembership{Senior Member,~IEEE}, 
	Brian~Quang-Huy~Nguyen,
	Ken-Yoong~Lee, 
	and Timo~Brestchneider % <-this % stops a space
\thanks{Thanh-Hai~Le, Brian~Quang-Huy~Nguyen and Ian~McLoughlin are with the School of Computer Engineering, 
Nanyang Technological University, Singapore,}% <-this % stops a space
\thanks{Ken-Yoong Lee and Timo Brestchneider are with EADS InnovationWorks Singapore.}% <-this % stops a space
\thanks{Manuscript received ?, 2010; revised ?.}}

\markboth{Transactions on Geoscience \& Remote Sensing,~Vol.~?, No.~?, April~2010}%
{ Le \MakeLowercase{\textit{et al.}}: SLC SAR speckle filtering using Homoskedastic effects of Logarithmic Transfomation }

\begin{document}

\maketitle

\begin{abstract}
This paper note the ability of Logarithmic Transformation to convert heteroskedasticity of SAR single look data to homoskedastic values. Under the assumption of homogeneity, the sampling distribution of variance in log-transformed domain is constant and independent to the underlying back scattering coefficient. Then the observable sample variance in the log-transformed domain can be used as a measure of spatial non-homogeneity, thus statistical test on log-transformed variance can be formed to detect heterogeneity in a window of analysis. Using clustering algorithms, a novel speckle filtering algorithm is proposed to partition a non-homogenous area,  detected by the statistical test, into homogenous areas with estimated back scattering coefficient values.
\end{abstract}

\begin{IEEEkeywords}
Synthetic aperture radar, stochastic-process, speckle-filtering, inverse-problems, homoskedasticity
\end{IEEEkeywords}

%\textbf{Keywords: } Synthetic Aperture Radar, speckle-filtering, homoskedasticity

\IEEEpeerreviewmaketitle

\section{Introduction}
%\section{Logarithmic Transformation of SAR Single look data}

The nature of SAR speckle is stochastics over homogenous areas and heteroskedastic heterogenously. Statistical models are used to describe how the underlying back-scattering coefficient ($\sigma$) affect the distribution of measured SAR data. Speckle filtering, by and large, are estimators attempting to the unknown coefficient from observable SAR signal $(A)$. 

SAR speckle-filtering can be and has been positioned within the context of estimation theory\cite{Touzi_2002_TGRS}. The statistical framework attempts to estimate unknown statistical parameters from observed data. The stages consist of statistical modelling, estimator development and evaluating of the performance of estimators. Estimators' are evaluated based on the bias and variance properties of their estimates. The performance evaluation is usually based qualitatively on some real data and quantitatively through simulated experiments. Due to the stochastic nature of simulation process, results of repeated experiments are normally preferred.

For about 30 years, speckle filtering has been an active research area, with new methods being introduced steadily (cite here some latest). That is because: eventhough, the statistical model within individual resolution cell is well established, the applicability of its corresponding estimators is restricted to homogenous areas. Practical images, however, are heterogenous. Crucially it is this spatial variation that is of high interest. This fact gives rise to the question that seemed obvious: how to call an analysis area heterogenous and subsequently what to do in the case of heterogeneity.

Various statistical models for heterogenous areas have been proposed (see \cite{Touzi_2002_TGRS} for a detailed review). Unfortunately, while most of the models highlight the multiplcative nature of sub-pixel or homogenous original SAR data, in extending the model to heterogenous images, virtually none has noted that spatial varition also gives rise to heteroskedasticity phenomena. Heteroskedastic, as explained in section ???, is defined as the dependence of conditional expected variance of original SAR data on the conditional expectation of its mean or equivalently its underlying back-scaterring coefficient $(\sigma)$. It is believed that heteroskedasticity give rise to serious negative impacts on various stages of speckle-filtering. 

In modelling, heteroskedasticity has direct consequences in the central question of homogeneity or heterogeneity. In normal images, the contrast or variance among neighboring pixels have been used to measure homogeneity. Unfortunately such techniques do not appear to be effective under the heteroskedastic condition of original SAR data. In SAR images, both of these measures are dependent on the underlying coefficient $(\sigma)$. This make the problem of estimating variance is equal to the problem of estimating the mean and is the same as the main problem: estimating $\sigma$. The fact give rise to the vicious circle in SAR speckle filtering.

Case in point can be illustrated in the recently published improved sigma filter\cite{Lee_TGRS_2009}. The technique determines outlying points as being too far away from the standard deviation. However, as is done in \cite{Lee_TGRS_2009}, to estimate standard deviation, an estimator of mean is required and used. It is interesting to note that the MMSE estimator used to estimate the mean in \cite{Lee_TGRS_2009}, itself alone, is a rather successful speckle-filter\cite{Lee_PAMI_1980}.

Heteroskedasticity also poses numerous challenges in designing an efficient estimator. Heteroskedasticity directly violates Gauss-Markov theorem's homoskedastic assumption. Thus it renders the efficiency of any naive Ordinary Least Square estimator in serious doubt \cite{Furno_1991_JStatCompSimul}. If the variance is known \textit{a priori}, it has been proven, cite Aitken, that a weighted mean estimator is the best linear unbiased estimator. Interestingly, as noted by Lopes \cite{Lopes_TGRS_1990}, most known common successful adaptive filters \cite{Lee_PAMI_1980} \cite{Kuan_1985_PAMI} \cite{Frost_PAMI_1982} does make use of weighted mean estimators. The caveat however is that in SAR speckle filtering, variance is not known a priori. And eventhough variance can be estimated from observable values. And as the vicious circle goes, estimating the variance is as good as estimating the underlying coefficient $\sigma$ itself.

Heteroskedastic also decrease the effectiveness of estimators by increasing the variance of even best possible estimations. This fact makes it tricky to evaluate the performance of different estimators, even if quantitative experiments results is reported. In fact several quantitative simulated experiments has been carried out to compare the performance of different estimators\cite{Lee_TGRS_2009}. However only results of single experiment were reported. Due to the stochastic nature of the simulated experiement process, more experiments should be done to report the effectiveness, ie. variance, of each estimators' results.

Last but certainly not least, is the bad impact of heteroskedastic on SAR image interpretation. Most of the task to be carried out in intepreting SAR images almost certainly involves target detection, target segmentation and/or target classification. All of these tasks requires a good similarity or discriminant functions. Foundation to all these is the need of a consistence sense of distance. Unfortunately, by definition heteroskedasticity provided inconsistent measures of distance. This inconsistent sense of distance coupled with the failure of the ordinary least square regression methods are believed to cause a large class of artificial neural networks as well as a number of other computational intelligence methods to underperform.

In this paper, we aim to neutralize these various impacts. The paper is organized as follows. Section ??? will be devoted to discussion of the heteroskedastic phenomena in the original per-pixel SAR amplitude and intensity model. Logarithmic transformation is extended to the model and the homoskedastic property of it is asserted. Section ??? will discuss how sampling distribution of variance in the log-transformed domain can be used to model various degrees of homogeneity. Section ??? will presents a novel speckle filtering algorithm developed from heterogenous inference made possible by statistical testing of sample variances. Section ??? will present evaluation of our estimator both quantitatively through simulated experiments and qualitatively on real-life images.

\section{SAR speckle statistic model}

Useful, robust estimators require un-assuming models. This section will describe our model for speckle filtering. The first part will explain the heteroskedastic nature of original amplitude and intensity SAR data. In extending the original model to log-transformed domain, care has been taken to rely solely on rigorous mathematical and statistical principles.

\subsection{Original Heteroskedastic Model}

SAR speckle phenomena is explained as the interference of many coherent but dephased back-scaterring components, each reflecting from different and distributed elementary scaterers \cite{Oliver_ProcIEEE_1963, Leith_ProcIEEE_1971}. These interference can be considered as a random walk on the 2D complex plane \cite{Goodman_JOptSocAm_76}.  The random nature of the process arises due to the unknown random location, height, distance and thus random phase of each elementary scaterer and its response.

Assuming the Central Limit Theorem is applicable \cite{Goodman_Springer_1975}, then the real part $A_r$ as well as the imaginary part $A_i$ of the observed SAR signal $A$ can be considered as random variables from uncorrelated Gaussian distributed stochastic processes with zero means and indentical variances $\sigma^2/2$  \cite{Lee_CRCPress_2009}. Their probability density function (pdf) is given as:

\begin{equation}
\label{eqn:component_signal_pdf}
pdf(A_x)=\frac{1}{\sqrt{\pi} \sigma} e^{\left( \frac{A_x^2}{\sigma^2} \right) }
\end{equation}

It then can be proved that the measurable amplitude $A=\sqrt{A_r^2+A_i^2}$ is a random variable of Rayleigh distributed and consequently intensity $I=A^2=(A_r^2+A_i^2)$ is a random variable of negative exponential distributed random process.

\begin{IEEEeqnarray}{l l l}
pdf(A) &=& \frac{2A}{\sigma^2}e^{ \left( -\frac{A^2}{\sigma^2} \right) }\\
pdf(I) &=& \frac{1}{\sigma^2}e^{\left( -\frac{I}{\sigma^2} \right) }
\end{IEEEeqnarray}

From statistical perspective, the multiplicative nature of amplitude and intensity data can be explained as follows. Consider two fixed, independent to $\sigma$ unit distributions given below:

\begin{IEEEeqnarray}{l l l}
pdf(A_1) &=& 2A_1 e^{ \left( A_1^2 \right) }\\
pdf(I_1) &=& e^{ \left( -I_1 \right) }
\end{IEEEeqnarray}

It is then trivial to prove that amplitude and intensity is simply a scaled version of these unit variables, ie. $A= \sigma A_1 $ and $I= \sigma^2 I_1 $. These relationships evidently manifest a mutiplicative nature. In fact, this condition has long been noted, but from different perspectives, in various SAR models including multiplicative model (cite JS Lee) and product model \cite{Jakeman_1980_JPhysAMathGen}.

If spatial homogeneity is defined as imaging scenes having the same back-scattering coefficient $\sigma$, then over a homogenous area, the measured values can then be considered as samples coming from a single stochastic process. Consequently, the population expected mean and variance of the four distributions are given in table \ref{tbl:orginal_sar_avg_var} 

\begin{table}[!h]
\caption{ Mean and Variance of Original SAR data are both related to the scale factor $\sigma$ }
\label{tbl:orginal_sar_avg_var}
\normalsize
\centering
%\begin{center}

\begin{tabular}{|l|l|}
\hline
Mean & Variance \\
\hline
$avg(A_1) = \frac{ \sqrt{\pi}}{2}$ & $var(A_1) = \frac{(4-\pi)}{4}$ \\
$avg(I_1) = 1$ & $var(I_1) = 1$ \\
$avg(A) = \frac{\sqrt{\pi}}{2} \cdot \sigma $ & $var(A) = \frac{(4-\pi)}{4} \cdot \sigma^2 $ \\
$avg(I) = \sigma^2 $ & $ var(I) = \sigma^4$ \\
\hline
\end{tabular}

%\end{center}
\end{table}

From the above analysis, it is evident that amplitude as well as intensity SAR data suffer from hetoroskedastic phenomena, which is defined as the dependence of conditional expected variance of SAR data on the conditional expectation of mean. In the context of speckle filtering, table \ref{tbl:orginal_sar_avg_var} indicates the vicious circle: estimating variance is equal to estimating the mean and is as good as main problem, ie. estimating the unknown parameter $\sigma$

The formulas above has long been noted. In fact while pioneering the estimation of the equivalent number of look (ENL) index, Lee et al has noted the ratio of expected standard deviation to mean is a constant in both cases (ie. $snr(A)=\sqrt{\frac{4}{\pi}-1}$ and $snr(I)=1$). Here, we just offer a different intepretation, which sets the stage for the discussion of logarithmic transformtion. 

\subsection{ Homoskedastic effect of Logarithmic Transformation }

We propose to consider base 2 logarithmic transformtion of SAR orignal random variables

\begin{IEEEeqnarray}{l l l l l}
L_{1^A} &=& \log_2(A_1) &=& L_{1^I} / 2 \\
L_A &=& \log_2(A) 	&=& L_{1^A} + \log_2\sigma \\
L_{1^I} &=& \log_2(I_1) \\
L_I &=& \log_2(I) 	&=& L_{1^I} + 2 \log_2\sigma
\end{IEEEeqnarray}

The equations above also highlight the relationships among random variables in log-transformed domain. Two conclusions become evident from these formula. Firstly, in log-transformed domain, working on amplitude or intensity will tend to give identical results. Secondly, the effects of converting multiplicative nature to additive nature of logarithmic transformation is clearly manifested. Bearing the relationship among the random variables in mind, it is then trivial to give the probability distribution of these log-transformed variable as below:

\begin{IEEEeqnarray}{l l l}
pdf(L_{1^A}) &=& 2 \cdot 2^{\left[ 2 L_{1^A} - 2^{2 L_{1^A}} \right]} \\
pdf(L_A) &=& 2 \cdot 2^{\left[ \left( 2 L_A - 2 \log_2 \sigma \right) - 2^{\left( 2 L_A - 2 \log_2 \sigma \right)} \right]} \\ 
pdf(L_{1^I}) &=& 2^{\left[ L_{1^I} - 2^{L_{1^I}} \right]} \\
pdf(L_I) &=& 2^{\left[ \left( L_I - 2 \log_2 \sigma \right) - 2^{\left( L_I - 2 \log_2 \sigma \right)} \right]} 
\end{IEEEeqnarray}

Noting that these distributions belong to Fisher-Tippet family, the population expected mean and variances are given in table \ref{tbl:sar_log_domain_avg_var}, with $\gamma$ being Euler-Mascheroni constant.

\begin{table}[!h]
\caption{ Mean and Variance of Log Transformed SAR values. It can be seen that the means are biased and the variances are no longer related to $\sigma$ }
\label{tbl:sar_log_domain_avg_var}
\normalsize
\centering
%\begin{center}

\begin{tabular}{|l|l|}
\hline
Mean & Variance \\
\hline
$avg(L_{1^A}) = \frac{ \gamma }{2} \cdot \frac{1}{\ln2}$ & $var(L_{1^A}) = \frac{ \pi ^2}{24} \cdot \frac{1}{(\ln2)^2}$ \\
$avg(L_{1^I}) = \gamma \cdot \frac{1}{\ln2} $ & $var(L_{1^I}) = \frac{ \pi ^2}{6} \cdot \frac{1}{(\ln2)^2} $ \\
$avg(L_A) = \frac{ \gamma }{2} \cdot \frac{1}{\ln2} + \log_2{\sigma}$ & $var(L_A) = \frac{ \pi ^2}{24} \cdot \frac{1}{(\ln2)^2}$ \\
$avg(L_I) = \gamma \cdot \frac{1}{\ln2} + 2 \log_2{\sigma}  $ & $ var(L_I) = \frac{ \pi ^2}{6} \cdot \frac{1}{(\ln2)^2}$ \\
\hline
\end{tabular}

%\end{center}
\end{table}

This result is consistence with finding by Arsenault \cite{Arsenault_JOptSocAm_1976}.  The main difference would be the use of base 2 logarithm which is prefered here for faster computation. Table \ref{tbl:sar_log_domain_avg_var} clearly confirms the condition of homoskedastic,  defined as  the independence of conditional expected variance on the conditional expectation of mean. 

Table \ref{tbl:sar_variables_properties} summarize the discussion so far. It can be seen that while original data, especially intensity values, should be prefered for multi-look processing, log-transformed domain, with its homoskedastic effects, offers consistence sense of dispersion and contrasts. Thus log transformation is proven here to be a homomorphic transformation allowing one to apply traditional linear, additive, least squared error regression signal processing (wavelet) and computational (neural networks) techniques into SAR data.

\begin{table*}[t]
\normalsize
%\hrulefill

%\begin{center}
\centering
\caption{ The properties of observable SAR random variables }
\label{tbl:sar_variables_properties}

\begin{tabular}{|l|l|l|l|}
\hline
 RV & Relationships  & Variance (skedasticity) & Mean (biasness) \\
\hline
$A$ & $A=\sigma A_1 $ & Heteroskedastic $var(A) = \frac{(4-\pi)}{4} \cdot \sigma^2 $ & Unbiased $avg(A) = \frac{\sqrt{\pi}}{2} \cdot \sigma $ \\
$I$ & $I=A^2=\sigma^2 I_1 $ & Heteroskedastic $ var(I) = \sigma^4$ & Unbiased $avg(I) = \sigma^2 $\\
$L_A$ & $L_A=\ln(A)=L_{1^A} + \log_2{\sigma}$ & Homoskedastic $var(L_A) = \frac{ \pi ^2}{24} \cdot \frac{1}{(\ln2)^2}$ & Biased $avg(L_A) = \frac{ \gamma }{2} \cdot \frac{1}{\ln2} + \log_2{\sigma}$ \\
$L_I$ & $L_I=\ln(I)=L_{1^I} + 2 \log_2{\sigma}$  & Homoskedastic $var(L_I) = \frac{ \pi ^2}{6} \cdot \frac{1}{(\ln2)^2}$ & Biased $avg(L_I) = \gamma \cdot \frac{1}{\ln2} + 2 \log_2{\sigma}  $ \\
\hline
\end{tabular}

%\end{center}
\end{table*}

The result here is experimentally confirmed by inspecting measured data from a known homogenous area. Fig. \ref{fig:modelled_response} plot the histogram of observable data within a homogenous area imaged by RADARSAT2 against modelled PDF response. The excellent agreement between this model and various experiments has long been noted and undisputed. This modelling has been very successful in eplaining speckle phenomena and verified through scientific experiments \cite{Ulaby_TGRS_1988}. 

\begin{figure}[h]
\centering
%\centerline{
\begin{tabular}{c}
	\subfloat[amplitude]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/amplitude_hist_model_pdf_scene1.eps} 	
		 \label{amplitude}
	} 
	\hfill	
	\subfloat[intensity]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/intensity_hist_model_pdf_scene1.eps} 	
		 \label{intensity}
	} \\
	\subfloat[log amplitude]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/log_amplitude_hist_model_pdf_scene1.eps} 	
		 \label{amplitude}
	} 
	\hfill	
	\subfloat[log intensity]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/log_intensity_hist_model_pdf_scene1.eps} 	
		 \label{intensity}
	} 
\end{tabular}
%}
\caption{Observed histogram in homogenous area and modelled pdf response}
\label{fig:modelled_response}
\end{figure}

Logarithmic transformation has long been noted \cite{Arsenault_JOptSocAm_1976} as being able to transform multiplicative nature of SAR data into additive nature but most of the focus has been on addition of multi-samples for multi-look processing in the log-transformed domain. However, in this paper the investigation of variance is the focus.

\section{Variance in Log-Transformed Domain}

\subsection{Consistent sense of distance in Log-Transformed domain}

The consistent sense is illustrated in this section from two different perspective. First, assume the back-scattering coefficient $\sigma$ is known a priori. Consider the random variable defined as the distance between an observable sample and its expected value. 

\begin{IEEEeqnarray}{l l l l l}
D^A &=& A &-& avg(A) \\
D^I &=& I &-& avg(I) \\
D^{L_A} &=& L_A &-& avg(L_A) \\
D^{L_I} &=& L_I &-& avg(L_I)
\end{IEEEeqnarray}

Noting the results from previous section, the pdf for these variable can be trivially given as 

\begin{IEEEeqnarray}{l l l}
pdf(D^A) &=& \frac{2\left( D^A + \frac{\sqrt{\pi}}{2} \sigma \right)}{\sigma^2}e^{ \left( -\frac{\left( D^A + \frac{\sqrt{\pi}}{2} \sigma \right)^2}{\sigma^2} \right) } \\
pdf(D^I) &=& \frac{1}{\sigma^2}e^{\left( -\frac{\left( D^I + \sigma^2 \right)}{\sigma^2} \right) } \\
pdf(D^{L_A}) &=& 2 \cdot 2^{\left[ \left( 2 D^{L_A} + 2 \frac{\gamma}{2 \ln2} \right) - 2^{\left( 2 D^{L_A} + 2 \frac{\gamma}{2 \ln2} \right)} \right]} \\
pdf(D^{L_I}) &=& 2^{\left[ \left( D^{L_I} + \frac{\gamma}{\ln2} \right) - 2^{\left( D^{L_I} + \frac{\gamma}{\ln2} \right)} \right]}
\end{IEEEeqnarray}

It is evident from the equations that these distributions are dependent on $\sigma$ in the original SAR data but is independent of it in the log-transformed domain.

From another perspective, given two adjacent resolution cells that is known to have the same but unknown back-scattering coefficient $\sigma$, consider the random variable defined as the contrast between two measured samples. 

\begin{IEEEeqnarray}{l l l l l}
C_A &=& A_1^{\sigma} &-& A_2^{\sigma} \\
C_I &=& I_1^{\sigma} &-& I_2^{\sigma} \\
C_{L^A} &=& L_1^{A_\sigma} &-& L_2^{A_\sigma} \\
C_{L^I} &=& L_1^{I_\sigma} &-& L_2^{I_\sigma} 
\end{IEEEeqnarray}

The pdf of these variables can be given as:

\begin{IEEEeqnarray}{l l l}
pdf(C_A) &=& \text{ TBD dependent on } \sigma \\
pdf(C_I) &=& \text{ TBD dependent on } \sigma \\
pdf(C_{L^A}) &=& 2 \frac{2^{\left(2 C_{L^A} \right)}}{1+2^{\left( 2 C_{L^A} \right)}} \ln2  \\
pdf(C_{L^I}) &=& \frac{2^{\left( C_{L^I} \right)}}{1+2^{\left( C_{L^I} \right)}} \ln2 
\end{IEEEeqnarray}

Noting that $C_x = D_1^x - D_2^x$, it should come as no suprise that the sense of contrast is consistent in log-domain but in-consistent in original domain. Figure \ref{fig:residual_as_noise} illustrate good agreement between the analytical pdf and observable histogram of distance and contrast of the same area in log-transformed intensity RADARSAT-2 image. Distance is calculated as the difference between each value point and the average value of the patch, while contrast is calculated as the difference between two horizontally adjacent values in log-transformed domain.

\begin{figure}[h!]
\centering
%\begin{tabular}{c}
	\subfloat[distance]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/log_intensity_dispersion_hist_model_pdf_scene1.eps} 	
		 \label{amplitude}
	} 
	\hfill
	\subfloat[contrast]{
		 \epsfxsize=1.5in
		 \epsfysize=1.5in
		 \epsffile{src/log_intensity_contrast_hist_model_pdf_scene1.eps} 	
		 \label{intensity}
	}
%\end{tabular}
\caption{Observed and modelled pdf of distance and contrast in homogenous log-transformed intensity images}
\label{fig:residual_as_noise}
\end{figure}

Given that real SAR images are heterogenous and the back-scattering coefficient is unknown, it is evident that the sense of observable distance in original SAR data differs, ie. is inconsistent, across different homogenous areas. The observable distance in log-transformed domain however is consistently the same across different homogenous areas. One possible benefit is that should the sigma filter be designed against the $pdf(C_{L^I})$, the scale estimator would no longer be required.

The conclusion in this section can be extended to larger number of pixels via the analysis of observable sample variance. 

\subsection{Sampling distribution of Variance in Log-Transformed domain}

From previous section result, two sample variance distribution ($V_x = C_x^2$) can be given analytically as

\begin{IEEEeqnarray}{l l l}
pdf(V_{L^I}) &=& 
	\frac{\ln2}{2} \frac{1}{\sqrt{V_{L^I}}} \left( \frac{2^{\sqrt{V_{L^I}}}}{\left( 1+2^{\sqrt{V_{L^I}}} \right)^2} + \frac{2^{-\sqrt{V_{L^I}}}}{\left( 1+2^{-\sqrt{V_{L^I}}} \right)^2} \right)\\
pdf(V_{L^A}) &=&
	\frac{\ln2}{4} \frac{1}{\sqrt{V_{L^A}}} \left( \frac{2^{\sqrt{4 V_{L^A}}}}{\left( 1+2^{\sqrt{4 V_{L^A}}} \right)^2} + \frac{2^{-\sqrt{4 V_{L^A}}}}{\left( 1+2^{-2\sqrt{V_{L^A}}} \right)^2} \right)
\end{IEEEeqnarray}

As log-transformed amplitude $L_A$ is simply half of log-transformed intensity values $L_I$, for brevity purposes, only results for log-transformed intensity values are presented.

It could be seen that, in log-transformed domain, not only the expected mean of observable variance is constant, the sampling distribution of this random variable is independent to $\sigma$. We have seen this analytically for two sample variances. For larger number of samples, Monte-Carlo simulation is used to visuallize pdf of sample variances.

\begin{enumerate}
\item Each input sample is a random and independent set of n samples each drawn from a single pdf given in $pdf(L_A)$ or $pdf(L_I)$
\item For each input set, the sample variance is calculated
\item For all input set, a histogram of calculated values is ploted
\item Number of input sets data generated is pre-determined as either 10000 (rough line) or 1000000 (smooth line)
\end{enumerate}

Here a figure giving simulated sampling distribution of variances with number of samples ranging from 2-9, analytical charts of 2, homogenous sample variances of 9. purpose to show match of is simulation with analytical, and simulation with measured data.

Here gives an example showing sample variance calculated from a SAR image.

\subsection{Statistical Hypothesis testing of homogenity}

It should be noted that all discussions in section ??? and ??? is based on the assumption of homogenous thus single stochastic process. In a real given analysis area in SAR images, such condition is unknown. Fortunately homoskedastic property, manifested in the sampling distribution of variance, provide a method for testing this homo/heterogeneity condition.

Under the null hypothesis that the given analysis area is homogenous, the theoretical distribution of sample variance has been given in previous sub-section. Thus given measured values and calculated log-transformed sample variance, we could accept / the notion that the given analysis area is heterogenous or homogenous with certain degrees of confidence. This form the basic of the statistical hypothesis test of homogenity.

The test statistics used is the sample variance. The pdf, and cdf of this test can be derived using Monte-Carlo simulation, illustrated above, and in the simple case of two samples can be given analytically. 

This give a figure of cdf for sampling distribution of variance. It is clear that if the sample variance is higher than a cut-off degree, the null hypothesis can be rejected with certain confidence.  

A table for cut-off values is also given here!

It should be noted there is a difference between accepting the null hypothesis and simply failing to reject it. However, in the section that followed, the decision rule in use is that if the sample variance is lower than the cut-off value, the null hypothesis is accepted. A  short analysis is given below to show that for the purpose of speckle-filtering the difference is negligible.

\section{Speckle Filtering in Log-Transformed domain}

Given an area of analysis, its non-homogesousity can be detected by the statistical hypothesis test decribed above. If that is likely to be the case, clustering algorithm can be applied to partition the area into smaller regions. The process can be applied iteratively until all partitions can be considered to be homogenous (accepting the null hypothesis). This form the basis of our proposed speckle filtering technique. 

\subsection{Homogenous Maximum Likelyhood Clustering}

Given a data set $X^e=\{x^e_1,...,x^e_n\}$ which follows exponential distribution $f^e(x)=\lambda e^{-\lambda x} $. We have already proven that the derivative data set $X^l=ln(X^e)$ follows Fisher Tippet distribution $f^l(x) = e^{d-e^d}$ with $d=x^l_i-\mu_0$. Next we are going to show analytically that, in the case of single-cluster, MLE of $\lambda$ from $X^e$ will equals MLE estimate of $\mu_0$ from $X^l$

Assume that the parameter of the distributions (ie. $\lambda$ or $\mu_0$ denoted as $\rho$) is known. MLE strategy maximize the conditional (likelyhood) probability $P(X^e | \rho )$. Product rule indicate that $P(X^e | \rho )=\displaystyle{\prod^n_{i=1}{P(x^e_i | \rho ) }}$. Maximize $P(X^e | \rho )$ also maximizes $\ln(P(X^e | \rho )) = \displaystyle{\sum^n_{i=1}{ \ln(P(x^e_i | \rho )) }}$. The parameter $\rho_0$ which maximizes likelyhood is found by setting 

\begin{equation}
   \displaystyle{ \frac{ \partial{ \ln[P( X^e | \rho )] }  }{\partial{ \rho }} 
= \displaystyle{\sum^n_{i=1}{ \frac{ \partial{ \ln(P(x^e_i | \rho )) } }{ \partial{ \rho } } }} 
= 0 }   
\end{equation}

Applying to log-transformed exponential distribution $X^l = \ln( X^e )$ we have
\begin{enumerate}
 \item $P(x^l_i | \mu) = e^{d - e^d}$ with $d=x^l-\mu $ thus $ \ln( P(x^l_i | \mu) ) = d - e^d$
 \item thus $ \displaystyle{\sum^n_{i=1}{ \frac{ \partial{ \ln(P(x^l_i | \mu )) } }{ \partial{ \mu } } } = \sum^n_{i=1}{[ e^{ (x^l_i - \mu) } - 1 ]} } =  \sum^n_{i=1}{ \frac{ e^{ x^l_i } }{ e^{ \mu }} } - n $
 \item MLE estimate $\mu_0$ of $\mu$ is found by setting $\displaystyle{ \sum^n_{i=1}{ \frac{ e^{ x^l_i } }{ e^{ \mu_0 }}  } - n = 0}$ Noting that $ e^{ \mu_0 }$ can be moved out of the sum, that gives us $\displaystyle{ \sum^n_{i=1}{ e^{ x^l_i } } = e^{\mu_0} n }$
 \item Note that $ e^{ x^l_i } = x^e_i $ thus $\displaystyle{ \sum^n_{i=1}{ e^{ x^l_i } } = \sum^n_{i=1}{x^e_i} }$ or $\displaystyle{ e^{\mu_0} = \frac{ \displaystyle{ \sum^n_{i=1}{x^e_i} }}{n} }$
 \item This result consistent with the relation $\lambda_0 = e^{-\mu_0}$ (QED)
\end{enumerate}

In the case of samples coming from multiple random processes clustering algorithm can be used to partition the analysis area into homogenous areas. An adapted version of clustering is given as

\begin{enumerate}
\item Initialize the cluster centers
\item Assign each sample to a center $C_i$ that has the highest probability $P(x|C_i)$ (winner take all)
\item Recalculate cluster centers for each $C_i$
\item Iterate above 3 steps until there is no more changes in membership from the last iteration
\end{enumerate}

It could be see that the partition here is rudimental. A more elaborate gradient-descent algorithm is mentioned next

\subsection{Heterogenous Gradient Descent Possibilistic Clustering}

In this section, it is assumed that the sample is known to be coming from different underlying back-scattering radar cross section. 

MLE strategy is to find $C$ by maximizing criterion function: $ \Upsilon = P(X|C) = \displaystyle \prod_{i=1}^n{P(x_i|C)}$ . Log-transform the MLE target function leads to $\log(\Upsilon)  = \displaystyle \sum_{i=1}^n{\log[P(x_i|C)]}$ which needs to be maximized

The standard iterative gradient descent learning where each cluster center is moved in a direction so as to maximize criterion function. To do that the ''moving force`` that is placed on each cluster $c_j$ is to be determined as

%\begin{equation}
\begin{eqnarray}
\label{eqn:single_cluster_move_force}
\frac{\partial(\log(\Upsilon))}{\partial(c_j)} 
	&=& \displaystyle \sum_{i=1}^n { \left( \frac{\partial(\log \left[ P(x_i|C) \right] }{\partial(c_j)} \right) }  
	\nonumber \\ 
	&=& \displaystyle \sum_{i=1}^n { \left( \frac{1}{P(x_i|C) } \cdot \frac{\partial(P(x_i|C))}{\partial(c_j)} \right) }
\end{eqnarray}
%\end{equation}

Eqn. \ref{eqn:single_cluster_move_force} is nice in the sense that the force applied on each cluster centers equals the sum of impacts from each data point. For each point, Eqn. (\ref{eqn:single_cluster_sample_derivative}) form the basis of the gradient descent MLE clustering

\begin{figure*}[t!]
\centering
\begin{eqnarray}
\label{eqn:single_cluster_sample_derivative}
\frac{\partial(P(x_i|C))}{\partial(c_j)} 
	= \left( \frac{ \partial(P(x_i|c_j) )}{\partial(c_j)}  \cdot \displaystyle \prod_{k=1,k \neq j}^c { \left[ 1-P(x_i|c_k) \right] } \right)
	%\nonumber \\
 	- \displaystyle \sum_{t=1,t \neq j}^c{ \left(  P(x_i|c_t) \cdot \displaystyle \prod_{k=1,k \neq j,t}^c { \left[ 1-P(x_i|c_k) \right] } \cdot \frac{ \partial(P(x_i|c_j) )}{\partial(c_j)}  \right) }
\end{eqnarray}
\end{figure*}

\begin{enumerate}
 \item The centers are initialized, possibly from the previous section
 \item Each cluster center $c_j$ is moved in proportion to $\frac{\partial(P(x_i|C))}{\partial(c_j)}$
\item The algorithm stops when the MLE $P(x_i|C)$ is no longer decreasing
\end{enumerate}

\section{Experiement Results and Discussions}%describe analysis and compare

\subsection{Evaluation on Simulated data}
The experiment setup is similar to \cite{Lee_TGRS_2009}. Evaluation is based on mean AND variance of total square error between estimated value and ground-truthed original values.

\subsection{Evaluation on Real Images}
Other algorithms' result and comparisions.

Comparisions criteria
\begin{enumerate}
\item Equivalent Number of Look 
\item Point target preservation
\item Texture preservation (if possible)
\end{enumerate}

\subsection{Discussions}

Issues to discuss
\begin{enumerate}
 \item Decision rule used in the hypothesis test
\item The confidence level used
\item The learning ratio used 
\end{enumerate}

%\appendices
\section{Conclusion}

The nature of SAR speckle is stochastics. The speckle filtering problem has been casted into statistical estimation theory framework. Speckle filtering, by and large, means the removal of stochastic component in SAR images. Various negative effects of heteroskedastic statiscal property of original SAR values are discussed. Log-transformation is shown to provide homoskedasticity in SAR modelling. Homoskedastic feature is shown here to help speckle-filtering, it might be possible that this features might also be found helpful in neural networks and other computational intelligence approaches.

The use of simulation and modelling in speckle filtering allow for faster simulation and evaluation of different underlying parameter estimations. Initial analysis can be done on simulated small data without the need of a real large image. Stochastics simulation also allows to evaluate efficiency of particular estimators. Inversely, qualititative requirements of speckle removal process can be mapped into specific requirements in designing speckle filters. As certain filters are good for certain frequency range, certain estimators are good in certain situations. Combination of estimators, aka. filter banks, may become achievable should their efficiency be estimated, and a situation-dependent measure indicating a prefered estimator is established and employed. Variance in log-transformed domain is shown to be such a measure.

The speckle filtering technique proposed in this paper has been shown to work for single-look SAR images. Multi-look processed image speckle filtering should be possible to use this technique with some minor adaptations. The filter is developed from an easy to understand winner take all clustering approach to a more fuzzy possibilistic clustering algorithm. Should the fuzzyness taken to a new level, an optimal filter which requires no clustering may be specified and designed.

% references section
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,article}

\end{document}
